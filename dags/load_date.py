from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import requests
import pandas as pd
import os
import time

# --- –ù–ê–°–¢–†–û–ô–ö–ò --- #
FRED_API_KEY = "11b3ddb5b880da8059e280fd7015dc35"

SERIES_TO_DOWNLOAD = [
    'CPIAUCSL',    # Consumer Price Index
    'UNRATE',      # Unemployment Rate  
    'FEDFUNDS',    # Federal Funds Rate
    'M2SL',        # Money Supply M2
    'SP500',       # S&P 500 Index
    'DGS10',       # 10-Year Treasury Yield
    'GDP',         # Gross Domestic Product
    'INDPRO',      # Industrial Production
]

CSV_FILE_PATH = "/opt/airflow/data/fred_daily_data.csv"

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã
REQUEST_TIMEOUT = 15  # —Å–µ–∫—É–Ω–¥
DELAY_BETWEEN_REQUESTS = 0.5  # —Å–µ–∫—É–Ω–¥
# ----------------------------- #

default_args = {
    'owner': 'you',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=2),
    'execution_timeout': timedelta(minutes=15),  # 15 –º–∏–Ω—É—Ç –º–∞–∫—Å
}

dag = DAG(
    'fred_daily_working',
    default_args=default_args,
    description='–†–∞–±–æ—á–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ FRED',
    schedule_interval='0 12 * * *',  # –ö–∞–∂–¥—ã–π –¥–µ–Ω—å –≤ 12:00 UTC
    catchup=False,
    tags=['fred', 'production'],
)

def download_fred_working(**kwargs):
    """
    –†–ê–ë–û–ß–ê–Ø –≤–µ—Ä—Å–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ FRED –¥–∞–Ω–Ω—ã—Ö
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ—Ç –∂–µ –ø–æ–¥—Ö–æ–¥, —á—Ç–æ –∏ —É—Å–ø–µ—à–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π DAG
    """
    print(f"‚è∞ –ù–∞—á–∞–ª–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {datetime.now()}")
    print(f"üìä –ë—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π: {len(SERIES_TO_DOWNLOAD)}")
    
    all_data = {}
    success_count = 0
    fail_count = 0
    
    for idx, series_id in enumerate(SERIES_TO_DOWNLOAD, 1):
        print(f"\n[{idx}/{len(SERIES_TO_DOWNLOAD)}] –ó–∞–≥—Ä—É–∂–∞—é {series_id}...")
        
        try:
            url = "https://api.stlouisfed.org/fred/series/observations"
            params = {
                'series_id': series_id,
                'api_key': FRED_API_KEY,
                'file_type': 'json',
                'observation_start': '2020-01-01',  # –ú–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å
                # –ë–µ–∑ limit - –±–µ—Ä—ë–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
            }
            
            # –ó–∞–ø—Ä–æ—Å —Å —Ç–∞–π–º–∞—É—Ç–æ–º
            response = requests.get(
                url, 
                params=params, 
                timeout=(10, REQUEST_TIMEOUT)
            )
            
            if response.status_code == 200:
                data = response.json()
                observations = data.get('observations', [])
                
                if observations:
                    # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ
                    dates = []
                    values = []
                    
                    for obs in observations:
                        val = obs['value']
                        if val != '.' and val is not None:
                            dates.append(obs['date'])
                            try:
                                values.append(float(val))
                            except ValueError:
                                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ
                    
                    if values:
                        # –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥
                        series = pd.Series(
                            values, 
                            index=pd.to_datetime(dates),
                            name=series_id
                        )
                        all_data[series_id] = series
                        success_count += 1
                        
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                        print(f"   ‚úÖ –£—Å–ø–µ—Ö: {len(values)} –∑–∞–ø–∏—Å–µ–π")
                        print(f"      –î–∏–∞–ø–∞–∑–æ–Ω: {dates[0]} - {dates[-1]}")
                        print(f"      –ü–æ—Å–ª–µ–¥–Ω–µ–µ: {values[-1]:.2f}")
                    else:
                        print(f"   ‚ö†Ô∏è –ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
                        fail_count += 1
                else:
                    print(f"   ‚ö†Ô∏è –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
                    fail_count += 1
                    
            elif response.status_code == 429:
                print(f"   ‚ö†Ô∏è Rate limit –ø—Ä–µ–≤—ã—à–µ–Ω. –ñ–¥—É 5 —Å–µ–∫—É–Ω–¥...")
                time.sleep(5)
                fail_count += 1
            else:
                print(f"   ‚ùå HTTP {response.status_code}: {response.text[:100]}")
                fail_count += 1
                
        except requests.exceptions.Timeout:
            print(f"   ‚è∞ –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞")
            fail_count += 1
        except Exception as e:
            print(f"   üí• –û—à–∏–±–∫–∞: {str(e)[:100]}")
            fail_count += 1
        
        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ —á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å API
        if idx < len(SERIES_TO_DOWNLOAD):
            time.sleep(DELAY_BETWEEN_REQUESTS)
    
    # --- –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í --- #
    print(f"\n{'='*50}")
    print(f"üìà –ò–¢–û–ì–ò –ó–ê–ì–†–£–ó–ö–ò:")
    print(f"   –£—Å–ø–µ—à–Ω–æ: {success_count}/{len(SERIES_TO_DOWNLOAD)}")
    print(f"   –ù–µ—É–¥–∞—á–Ω–æ: {fail_count}/{len(SERIES_TO_DOWNLOAD)}")
    print(f"{'='*50}\n")
    
    if all_data:
        # –°–æ–±–∏—Ä–∞–µ–º DataFrame
        df = pd.DataFrame(all_data)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ
        df = df.sort_index()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        df['_download_date'] = datetime.now().strftime('%Y-%m-%d')
        df['_download_timestamp'] = datetime.now()
        
        # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –Ω–µ—Ç
        os.makedirs(os.path.dirname(CSV_FILE_PATH), exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
        df.to_csv(CSV_FILE_PATH, index=True)
        
        print(f"üíæ –î–ê–ù–ù–´–ï –°–û–•–†–ê–ù–ï–ù–´:")
        print(f"   –§–∞–π–ª: {CSV_FILE_PATH}")
        print(f"   –†–∞–∑–º–µ—Ä: {len(df)} —Å—Ç—Ä–æ–∫ √ó {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
        print(f"   –û–±—ä—ë–º: {os.path.getsize(CSV_FILE_PATH) / 1024:.1f} KB")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        print(f"\nüìä –ü–û–°–õ–ï–î–ù–ò–ï –ó–ù–ê–ß–ï–ù–ò–Ø:")
        for col in list(df.columns)[:6]:  # –ü–æ–∫–∞–∂–µ–º –ø–µ—Ä–≤—ã–µ 6 –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π
            if col.startswith('_'):
                continue
            last_val = df[col].iloc[-1] if not df[col].isna().all() else 'N/A'
            print(f"   {col}: {last_val}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–∫–∂–µ summary —Ñ–∞–π–ª
        summary_path = CSV_FILE_PATH.replace('.csv', '_summary.txt')
        with open(summary_path, 'w') as f:
            f.write(f"FRED Data Download Summary\n")
            f.write(f"Date: {datetime.now()}\n")
            f.write(f"Total series: {len(SERIES_TO_DOWNLOAD)}\n")
            f.write(f"Successfully downloaded: {success_count}\n")
            f.write(f"Failed: {fail_count}\n")
            f.write(f"Data shape: {df.shape}\n")
        
        print(f"\nüìù Summary —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {summary_path}")
        
        return CSV_FILE_PATH
        
    else:
        print("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è!")
        
        # –°–æ–∑–¥–∞—ë–º —Ñ–∞–π–ª —Å –æ—à–∏–±–∫–æ–π
        error_path = CSV_FILE_PATH.replace('.csv', '_ERROR.txt')
        with open(error_path, 'w') as f:
            f.write(f"FRED Download FAILED\n")
            f.write(f"Time: {datetime.now()}\n")
            f.write(f"All {len(SERIES_TO_DOWNLOAD)} series failed\n")
        
        print(f"üìù –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∞–Ω–∞ –≤: {error_path}")
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤—ã–∑—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ —á—Ç–æ–±—ã Airflow –∑–Ω–∞–ª –æ –ø—Ä–æ–±–ª–µ–º–µ
        raise Exception(f"Failed to download any FRED data. Check {error_path}")

# –°–æ–∑–¥–∞—ë–º –∑–∞–¥–∞—á—É
download_task = PythonOperator(
    task_id='download_fred_data',
    python_callable=download_fred_working,
    dag=dag,
)

download_task